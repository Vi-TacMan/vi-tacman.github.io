<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="keywords"
    content="Articulated object manipulation, multi-modal perception, and tactile-informed manipulation" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    Vi-TacMan: Articulated Object Manipulation via Vision and Touch
  </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://player.vimeo.com/api/player.js"></script>
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>

  <!-- MathJax -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Related Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://arxiv.org/abs/2403.01694" target="_blank">
              [T-RO24] Tac-Man: Tactile-Informed Prior-Free Manipulation of Articulated Objects
            </a>
            <a class="navbar-item" href="https://arxiv.org/abs/2508.02204" target="_blank">
              [ArXiv] TacMan-Turbo: Proactive Tactile Control for Articulated Object Manipulation
            </a>
            <a class="navbar-item" href="https://www.nature.com/articles/s42256-025-01053-3" target="_blank">
              [NMI25] F-TAC Hand: A Biomimetic Hand Featuring High-Resolution Tactile Sensing
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>

  <section class="hero">
    <div class="hero-body" style="padding-bottom: 0;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Vi-TacMan: Articulated Object Manipulation via Vision and Touch
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://lycui.com/" target="_blank">Leiyao Cui</a><sup>1,2 *</sup>,
              </span>
              <span class="author-block">
                <a href="https://zihangzhao.com/" target="_blank">Zihang Zhao</a><sup>2,3,4,5 * ✉️</sup>,
              </span>
              <span class="author-block">
                <a href="https://siruixie.github.io/" target="_blank">Sirui Xie</a><sup>2,3,4 *</sup>,
              </span>
              <span class="author-block">
                Wenhuan Zhang<sup>6 *</sup>,
              </span>
              <span class="author-block">
                Zhi Han<sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a><sup>2,3,4 ✉️</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup> University of Chinese Academy of Sciences
              </span>
              <br>
              <span class="author-block">
                <sup>2</sup> Institute for Artificial Intelligence, Peking University
              </span>
              <br>
              <span class="author-block">
                <sup>3</sup> School of Psychological and Cognitive Sciences, Peking University
              </span>
              <br>
              <span class="author-block">
                <sup>4</sup> Beijing Key Laboratory of Behavior and Mental Health, Peking University
              </span>
              <br>
              <span class="author-block">
                <sup>5</sup> LeapZenith AI Research
              </span>
              <span class="author-block">
                <sup>6</sup> The work was done during his internship at Peking University
              </span>
              <span class="eql-cntrb"><small><br /><sup>*</sup> Equal Contributor</small></span>
              <span class="eql-cntrb"><small>&nbsp&nbsp&nbsp&nbsp&nbsp<sup>✉️</sup> Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <img src="static/images/teaser.png" style="width:100%; max-width:100%;">
          <div class="content has-text-justified">
            <p>
              Vi-TacMan exploits the complementary strengths of vision and touch to manipulate previously unseen
              articulated objects: vision, with its global receptive field, proposes a coarse grasp and an initial
              interaction direction, which sufficiently activates the tactile-informed controller that leverages local
              contact information to achieve precise and robust manipulation.
            </p>
          </div>
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Autonomous manipulation of articulated objects represents a basic skill for robots deployed in human
              environments.
              Current vision-based methods can infer object hidden kinematics, but their estimates are sometimes
              imprecise in driving reliable actions, especially on previously unseen objects.
              Tactile methods, on the other hand, excel once contact is made, yet they require a reasonable initial
              guess about where and how to interact.
              This observation suggests a natural division of labor: vision provides global, coarse guidance, while
              touch delivers precise, robust execution.
              Building on this complementarity, we propose a systematic approach, Vi-TacMan, which uses vision to plan
              and touch to control.
              We begin by training a vision module that accurately detects holdable and movable parts.
              Once identified, these parts are then segmented for further processing.
              From these detections, the system proposes feasible grasps along with a coarse interaction direction
              modeled by a von Mises-Fisher (vMF) distribution.
              To enhance directional reasoning, we explicitly incorporate surface normals on movable regions as a
              geometric prior.
              This inductive bias clarifies the expected motion and improves generalization to unseen objects, yielding
              significant gains over baseline methods (all $p$-values less than 0.0001).
              Finally, seeded with the vision-derived grasp and motion direction, a tactile-informed controller
              establishes and maintains stable interactions, enabling reliable execution of the manipulation.
              Real-world object experiments on diverse objects further confirm reliable manipulation without explicit
              kinematic models.
              These findings establish a paradigm for multi-modal robotic perception that could advance autonomous
              systems operating in complex, unstructured environments.
            </p>
          </div>
          <div style="padding:56.25% 0 0 0;position:relative;"><iframe
              src="https://player.vimeo.com/video/1122885741?h=af48f7df9f&badge=0&autopause=0&player_id=0&app_id=58479"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen
              style="position:absolute;top:0;left:0;width:100%;height:100%;" title="Supplementary Video"></iframe>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Pipeline -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title">Pipeline Overview</h2>
          <div class="content is-centered has-text-justified">
            <p class="math-content">
              When manipulating a previously unseen articulated object placed at a random position, the Vi-TacMan
              framework begins by capturing an RGB-D image of the scene. Utilizing its global receptive field, the
              vision module identifies holdable and movable parts, segments them from the surrounding environment, and
              proposes both a feasible grasp configuration and a coarse motion direction. These visual cues provide
              sufficient information for the tactile-informed controller to establish stable contact with the object.
              Guided by the inferred motion direction, the controller maintains this stable contact throughout the
              manipulation process, ensuring precise and robust execution.
            </p>
          </div>
          <div style="padding:56.25% 0 0 0;position:relative;"><iframe
              src="https://player.vimeo.com/video/1122886607?h=13262734b2&badge=0&autopause=0&player_id=0&app_id=58479"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen
              style="position:absolute;top:0;left:0;width:100%;height:100%;" title="Supplementary Video"></iframe>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End pipeline -->

  <!-- Simulation results -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title">Modules and Results</h2>
          <h2 class="subsection-title">Detection and Segmentation Module</h2>
          <div class="content is-centered has-text-justified">
            <p class="math-content">
              To identify holdable and movable regions, we first train a transformer-based detector that leverages
              DinoV3 visual features. The detector achieves a notably high mAP of 0.86 on the test set and generalizes
              seamlessly to real-world images. The model architecture and checkpoints are released with our code. The
              detection results are then used to prompt SAM2 for segmentation. In the following examples, blue indicates
              "holdable" parts, while orange indicates "movable" parts.
            </p>
            <div class="container is-max-desktop">
              <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                  <img src="static/images/dect_and_seg/bb16.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask16.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb17.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask17.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb18.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask18.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb19.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask19.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb20.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask20.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb11.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask11.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb12.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask12.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb13.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask13.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb14.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask14.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb15.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask15.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb6.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask6.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb7.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask7.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb8.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask8.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb9.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask9.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb10.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask10.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb1.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask1.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb2.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask2.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb3.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask3.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb4.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask4.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
                <div class="item">
                  <img src="static/images/dect_and_seg/bb5.png" loading="lazy" style="width:50%; max-width:50%;" />
                  <img src="static/images/dect_and_seg/mask5.png" loading="lazy" style="width:50%; max-width:50%;" />
                </div>
              </div>
            </div>
          </div>
          <h2 class="subsection-title">Grasp Generation Module</h2>
          <div class="content is-centered has-text-justified">
            <p class="math-content">
              With the movable and holdable masks defined, the next step is to establish a stable grasp on the handle.
              Recent advances have demonstrated the effectiveness of parallel grippers for object grasping, even within
              cluttered environments, like AnyGrasp. However, the
              handle-grasping problem can be largely simplified in our setting. To this end, we adopt a sampling-based
              method. The grasp region is restricted to the
              holdable area, and the grasping point $\boldsymbol{g}$ is defined as the centroid of this region, which
              also determines the gripper translation. We then sample gripper rotations to identify one that yields a
              collision-free grasp with minimal gripper width.
            </p>
            <div class="model-container" id="model-compare-wrapper">
              <!-- Model 1 Viewer with Label -->
              <div class="model-wrapper-comparison">
                <div class="model-label">Ours</div>
                <model-viewer id="modelViewerComparison1" loading="eager" touch-action="pan-y"
                  environment-image="legacy" zoom-sensitivity="0.2" camera-controls="" disable-tap=""
                  min-camera-orbit="auto auto 1m" max-camera-orbit="auto auto 10m" interaction-prompt="none"
                  shadow-intensity="0.3" ar="" style="width: 100%; height: 100%;" ar-status="not-presenting">
                </model-viewer>
              </div>
              <!-- Model 2 Viewer with Label -->
              <div class="model-wrapper-comparison">
                <div class="tips-wrapper">
                  <div class="tips-icon">💡Tips</div>
                  <div class="tips-text">
                    <p>● Scroll to zoom in/out</p>
                    <p>● Drag to rotate</p>
                    <p>● Press "shift" and drag to pan</p>
                  </div>
                </div>
                <div class="model-label">AnyGrasp</div>
                <model-viewer id="modelViewerComparison2" loading="eager" touch-action="pan-y"
                  environment-image="legacy" zoom-sensitivity="0.2" camera-controls="" disable-tap=""
                  min-camera-orbit="auto auto 1m" max-camera-orbit="auto auto 10m" interaction-prompt="none"
                  shadow-intensity="0.3" ar="" style="width: 100%; height: 100%;" ar-status="not-presenting">
                </model-viewer>
              </div>
            </div>
            <div class="selection-panel" id="comparisonSelectionPanel"
              style="width: 90% !important; aspect-ratio: 3.8/1; margin-left: 5%;">
              <img class="selectable-image selected" name="drawer" src="static/comparison/drawer/color_no_bg.png">
              <img class="selectable-image" name="large_cabinet" src="static/comparison/large_cabinet/color_no_bg.png">
              <img class="selectable-image" name="microwave" src="static/comparison/microwave/color_no_bg.png">
              <img class="selectable-image" name="small_cabinet" src="static/comparison/small_cabinet/color_no_bg.png">
            </div>
            <p class="math-content">
              We show the comparison of grasp poses generated by our method and AnyGrasp.
              Handles of articulated objects typically constrain the grasp pose to an 1-DoF configuration.
              However, the 6-DoF predictions from AnyGrasp often yield tilted poses that cannot provide reliable grasps.
              In contrast, our method exploits surface normals as priors to guide the grasp approaching direction and
              further refines the in-plane rotation through collision detection, resulting in more reliable
              grasp poses.
            </p>
          </div>
          <h2 class="subsection-title">Direction Inference Module</h2>
          <div class="content is-centered has-text-justified">
            <p class="math-content">
              We model the distribution of interaction directions given visual observations by sampling different point
              groups in the movable region and inferring the corresponding point displacements after a small
              perturbation. Each group yields a rigid transformation that, combined with the selected grasping point,
              converts to a possible direction. In the ideal case, all directions would equal the ground truth and the
              distribution would degenerate to a Dirac delta function. However, due to uncertainty in visual
              observations, the directions are typically scattered around the ground truth. We fit a von Mises-Fisher
              (vMF) distribution to these samples to model this uncertainty, where the mean direction achieves the
              highest probability density. In practice, we compute the Fréchet mean of the sampled directions under the
              geodesic metric (arc length) on the sphere, yielding an unbiased estimator of the mean.
            </p>
            <img src="static/images/sim.png" style="width:100%; max-width:100%;">
            <p class="math-content">
              We illustrate the approach using four representative objects, one from each test category.
              For each object, we present the obtained samples, the fitted vMF distribution, the ground truth, and the
              predictions of the three baseline methods.
              By fitting the distribution and incorporating the surface normal as an inductive bias, our proposed
              method
              demonstrates greater robustness to high uncertainties when encountering previously unseen objects.
            </p>
            <figure class="figure-side left has-text-centered">
              <figcaption><br><strong>Quantitative results of direction estimates on unseen object
                  categories.</strong> We report prediction errors from four methods over 5,836 test samples drawn from
                categories not seen during training. Our method, which uses surface normals as an inductive bias,
                achieves a significant performance gain over the baselines. The violin plots show the error
                distributions: the outer shape is the kernel density estimate; the white dot is the median; the
                thick bar denotes the interquartile range (IQR); and the whiskers extend to \(1.5\times\) IQR beyond the
                quartiles. **** indicates \(p\)-value < 0.0001. </figcaption>
                  <img src="static/images/direction.png">
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End simulation results -->

  <!-- Real-world results -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title">Real-World Validations</h2>
          <div style="padding:56.25% 0 0 0;position:relative;"><iframe
              src="https://player.vimeo.com/video/1122886898?h=1bcb6577b7&badge=0&autopause=0&player_id=0&app_id=58479"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen
              style="position:absolute;top:0;left:0;width:100%;height:100%;" title="Supplementary Video"></iframe>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End real-world results -->

  <!-- Paper acknowledgment -->
  <section class="section hero" style="padding-top: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Acknowledgment</h2>
          <div class="content has-text-justified">
            <p>This work is supported in part by the National Science and Technology Innovation 2030 Major Program
              (2025ZD0219402), the National Natural Science Foundation of China (62376009), the Beijing Nova Program,
              the State Key Lab of General AI at Peking University, the PKU-BingJi Joint Laboratory for Artificial
              Intelligence, and the National Comprehensive Experimental Base for Governance of Intelligent Society,
              Wuhan East Lake High-Tech Development Zone.</p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper acknowledgment -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project
                Page Template</a>
              which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project
              page.<br />
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons
                Attribution-ShareAlike 4.0 International
                License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script src="static/js/comparison.js"></script>

</body>

</html>